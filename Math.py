import jittor as jt
from Utils.misc import setup_for_distributed, get_rank, get_world_size, add_weight_decay, save_model
from Models.LLaMA_Adapter import LLaMA_adapter
from Data.MathDataset import MathDataset
from Utils.EarlyStopper import EarlyStopper

import argparse
import datetime
import json
import numpy as np
import os
import time
from typing import List

from pathlib import Path
from Utils.misc import MetricLogger, SmoothedValue
from Train.engine_finetune import train_one_epoch
from tensorboardX import SummaryWriter

jt.flags.log_silent = 1
jt.flags.auto_mixed_precision_level = 1

def str2bool(v):
    """å­—ç¬¦ä¸²è½¬å¸ƒå°”å€¼"""
    if isinstance(v, bool):
        return v
    if v.lower() in ('yes', 'true', 't', 'y', '1'):
        return True
    elif v.lower() in ('no', 'false', 'f', 'n', '0'):
        return False
    else:
        raise argparse.ArgumentTypeError('Boolean value expected.')


def get_args_parser():
    """å®šä¹‰å‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser('llama_adapterV2 finetuning', add_help=False)
    
    # åŸºç¡€è®­ç»ƒå‚æ•°
    parser.add_argument('--batch_size', default=64, type=int, help='æ‰¹æ¬¡å¤§å°')
    parser.add_argument('--epochs', default=400, type=int, help='è®­ç»ƒè½®æ•°')
    parser.add_argument('--accum_iter', default=1, type=int, help='æ¢¯åº¦ç´¯ç§¯æ¬¡æ•°')
    parser.add_argument('--early_stop_patience', default=5, type=int, help='æ—©åœè½®æ•°')

    # æ¨¡å‹å‚æ•°
    parser.add_argument('--llama_path', default='/path/to/llama', type=str, help='LLaMAæ¨¡å‹è·¯å¾„')
    parser.add_argument('--max_seq_len', default=512, type=int, help='æœ€å¤§åºåˆ—é•¿åº¦')
    parser.add_argument('--max_batch_size', default=32, type=int, help='æ¨ç†æ‰¹æ¬¡å¤§å°')

    # LoRAå‚æ•°
    parser.add_argument('--w_bias', default=False, type=bool, help='æ˜¯å¦å¾®è°ƒåç½®é¡¹')
    parser.add_argument('--lora_layers', default='0-0', type=str, help='LoRAå±‚èŒƒå›´')
    parser.add_argument('--lora_rank', default=16, type=int, help='LoRAç§©')
    parser.add_argument('--lora_targets', default='Q,V', type=str, help='LoRAç›®æ ‡æ¨¡å—')
    parser.add_argument('--lora_alpha', default=8, type=int, help='LoRAç¼©æ”¾å‚æ•°')
    parser.add_argument('--expert_num', default=1, type=int, help='ä¸“å®¶æ•°é‡')
    parser.add_argument('--hydra_moe', type=str2bool, nargs='?', const=True, default=False, help='å¯ç”¨Hydra MoE')
    parser.add_argument('--expert_weight', type=str2bool, nargs='?', const=True, default=False, help='ä¸“å®¶æƒé‡è®¾ç½®')

    # Prompt Tuningå‚æ•°
    parser.add_argument('--prompt_layers', default='0-0', type=str, help='Promptå±‚èŒƒå›´')
    parser.add_argument('--prompt_len', default=10, type=int, help='Prompté•¿åº¦')

    # Parallel Adapterå‚æ•°
    parser.add_argument('--p_adapter_layers', default='0-0', type=str, help='Parallel Adapterå±‚èŒƒå›´')
    parser.add_argument('--p_adapter_size', default=16, type=int, help='Parallel Adapteréšè—å±‚å¤§å°')
    parser.add_argument('--p_adapter_hydra', type=str2bool, nargs='?', const=True, default=False, help='Parallel Adapter Hydraæ¨¡å¼')

    # Adapterè·¯ç”±å‚æ•°
    parser.add_argument('--swi_x', default=0, type=int, help='é€‚é…å™¨è·¯ç”±å‚æ•°')
    parser.add_argument('--sparse', default=True, type=bool, help='æ˜¯å¦ç¨€ç–è·¯ç”±')
    parser.add_argument('--if_trainable_gamma', default=True, type=bool, help='é˜ˆå€¼æ˜¯å¦å¯è®­ç»ƒ')
    parser.add_argument('--gamma', default=0.5, type=float, help='é˜ˆå€¼')

    # æ•°æ®é›†å‚æ•°
    parser.add_argument("--data_path", default="", type=str, help="è®­ç»ƒæ•°æ®è·¯å¾„")
    parser.add_argument("--val_data_path", default="", type=str, help="éªŒè¯æ•°æ®è·¯å¾„")

    # ä¼˜åŒ–å™¨å‚æ•°
    parser.add_argument('--weight_decay', type=float, default=0.05, help='æƒé‡è¡°å‡')
    parser.add_argument('--lr', type=float, default=None, metavar='LR', help='å­¦ä¹ ç‡')
    parser.add_argument('--blr', type=float, default=1e-3, metavar='LR', help='åŸºç¡€å­¦ä¹ ç‡')
    parser.add_argument('--min_lr', type=float, default=0., metavar='LR', help='æœ€å°å­¦ä¹ ç‡')
    parser.add_argument('--warmup_epochs', type=float, default=1, metavar='N', help='é¢„çƒ­è½®æ•°')

    # è¾“å‡ºå’Œè®¾å¤‡å‚æ•°
    parser.add_argument('--output_dir', default='./output', help='è¾“å‡ºç›®å½•')
    parser.add_argument('--device', default='cuda', help='è®­ç»ƒè®¾å¤‡')
    parser.add_argument('--seed', default=0, type=int, help='éšæœºç§å­')
    parser.add_argument('--start_epoch', default=0, type=int, metavar='N', help='å¼€å§‹è½®æ•°')

    # æ–­ç‚¹ç»­è®­å‚æ•°
    parser.add_argument('--trainable_params', type=str2bool, nargs='?', const=True, default=False, help='æ˜¯å¦æ–­ç‚¹ç»­è®­')
    parser.add_argument('--checkpoint_path', type=str, default=None, help='æ–­ç‚¹ç»­è®­è·¯å¾„')

    return parser


def prepare_args(data_path):
    default_cli = [
        '--llama_path', '/hy-tmp/LLaMA/original',
        '--data_path',  data_path,
        '--device', 'cuda',
        '--batch_size', '16',
        '--epochs', '5',
        '--max_seq_len', '300',
        '--lr', '5e-5',
        '--accum_iter', '1',
        '--lora_layers', '0-32',
        '--lora_rank', '8',
        '--lora_targets', 'Q,K,V,O',
        '--prompt_layers', '0-32',
        '--p_adapter_layers', '0-32',
        '--swi_x', '1',
        '--seed', '0',
        '--output_dir', '/root/MoA_Jittor/Output',
        '--early_stop_patience', '5',
        '--sparse', 'True',
        '--if_trainable_gamma', 'True',
        '--gamma', '0.5',
        '--trainable_params', 'False',
        '--checkpoint_path', '/root/MoA_Jittor/Output/LoRA_0-32_r8_a8_Q,K,V,O_Prompt_0-32_len10_PAdapter_0-32_size16_swi_x1_lr5e-5_bs2_Debug_seed0/checkpoint-0.pth'
    ]
    args = get_args_parser().parse_args(default_cli)

    if args.output_dir:
        Path(args.output_dir).mkdir(parents=True, exist_ok=True)

    return args

def finetune(args_list: List[argparse.Namespace], model : LLaMA_adapter):
    args = args_list[0]
    llama_tokenzier_path = os.path.join(args.llama_path, 'tokenizer.model')
    # åˆ›å»ºæ•°æ®é›†
    dataset_train = MathDataset(
        data_paths=[args.data_path for args in args_list], 
        tokenizer_path=llama_tokenzier_path, 
        max_tokens=args.max_seq_len, 
        partition="train",
        val_ratio=0.05,
        batch_size=args.batch_size,
        shuffle=True,
        drop_last=True,
    )
    dataset_train.datainit()

    dataset_val = MathDataset(
        data_paths=[args.data_path for args in args_list], 
        tokenizer_path=llama_tokenzier_path, 
        max_tokens=args.max_seq_len, 
        partition="val",
        val_ratio=0.05,
        batch_size=args.batch_size,
        shuffle=True,
        drop_last=True,
    )
    dataset_val.datainit()

    print('====================================================== æ•°æ®é›†å¤§å° =======================================================')
    print(f'è®­ç»ƒé›†å¤§å°: {len(dataset_train)}')
    print(f'éªŒè¯é›†å¤§å°: {len(dataset_val)}')

    # é…ç½®ä¼˜åŒ–å™¨
    param_groups = add_weight_decay(model, args.weight_decay)
    optimizer = jt.optim.AdamW(param_groups, lr=args.lr, betas=(0.9, 0.95))
    early_stopper = EarlyStopper(patience=args.early_stop_patience, min_delta=0.01, mode='min')

    if args.trainable_params:
        checkpoint = jt.load(args.checkpoint_path)
        args.start_epoch = checkpoint['epoch']
        data_iter_step = int(checkpoint['trainable_params'][0].split('_')[-1])
        model.load_state_dict(checkpoint['trainable_params'][1])
        optimizer.load_state_dict(checkpoint['optimizer'])
        early_stopper.load_state(checkpoint['early_stopper'])
        metric_logger = MetricLogger()
        metric_logger.load_state_dict(checkpoint['metric_logger'])
        val_metric_logger = MetricLogger()
        val_metric_logger.load_state_dict(checkpoint['val_metric_logger'])
        print(f'================================================== æ–­ç‚¹ç»­è®­ =======================================================')
        print(f'æ–­ç‚¹ç»­è®­è½®æ•°: {args.start_epoch}')
        print(f'æ–­ç‚¹ç»­è®­æ­¥æ•°: {data_iter_step}')
    else:       
        metric_logger = MetricLogger(delimiter="  ")
        metric_logger.add_meter('lr', SmoothedValue(window_size=1, fmt='{value:.6f}'))
        metric_logger.add_meter('closs', SmoothedValue(window_size=5, fmt='{median:.6f}'))
        metric_logger.add_meter('mloss', SmoothedValue(window_size=5, fmt='{median:.6f}'))

        val_metric_logger = MetricLogger(delimiter="  ")
        val_metric_logger.add_meter('loss', SmoothedValue(window_size=1, fmt='{value:.6f}'))

        data_iter_step = -1

    # é…ç½®æ—¥å¿—
    args.log_dir = os.path.join(args.output_dir, 'log')
    
    if get_rank() == 0 and args.log_dir is not None:
        os.makedirs(args.log_dir, exist_ok=True)
        log_writer = SummaryWriter(log_dir=args.log_dir)
    else:
        log_writer = None

    # å¼€å§‹è®­ç»ƒ
    print(f"============================================ å¼€å§‹è®­ç»ƒ  ==================================================")
    start_time = time.time()
    
    for epoch in range(args.start_epoch, args.epochs):
        # è®­ç»ƒä¸€ä¸ªepoch
        train_stats, val_stats, early_stopped = train_one_epoch(
            model=model,
            data_loader=dataset_train,
            val_loader=dataset_val,
            optimizer=optimizer,
            epoch=epoch,
            data_iter_step=data_iter_step,
            args=args, 
            log_writer=log_writer,
            early_stopper=early_stopper,
            val_interval=50,
            metric_logger=metric_logger,
            val_metric_logger=val_metric_logger
        )

        data_iter_step = -1

        # ä¿å­˜æ¨¡å‹
        if args.output_dir:
            save_model(args=args, epoch=epoch, model=model, optimizer=optimizer, early_stopper=early_stopper, log_writer=log_writer, metric_logger=metric_logger, val_metric_logger=val_metric_logger)

        # è®°å½•æ—¥å¿—
        log_stats = {**{f'train_{k}': v for k, v in train_stats.items()},
                     'epoch': epoch,
                     **{f'val_{k}': v for k, v in val_stats.items()}}

        if args.output_dir and get_rank() == 0:
            if log_writer is not None:
                log_writer.flush()
            
            with open(os.path.join(args.output_dir, "log.txt"), mode="a", encoding="utf-8") as f:
                f.write(json.dumps(log_stats) + "\n")

        if early_stopped:
            print(f'================================================== æ—©åœ =======================================================')
            break

        print(f'================================================== ç¬¬ {epoch} è½®è®­ç»ƒå®Œæˆ =======================================================')

    # è®­ç»ƒå®Œæˆ
    total_time = time.time() - start_time
    total_time_str = str(datetime.timedelta(seconds=int(total_time)))
    print('è®­ç»ƒæ—¶é—´: {}'.format(total_time_str))
    print(f'================================================  è®­ç»ƒå®Œæˆ =====================================================')


def main(args_list):
    # è®¾ç½®è®¾å¤‡
    args = args_list[0]
    if args.device == 'cuda':
        jt.flags.use_cuda = 1
    
    if jt.in_mpi:
        setup_for_distributed(jt.rank == 0)

    print(f'==================================================== Job Start ====================================================')
    print('å·¥ä½œæ–‡ä»¶å¤¹: {}'.format(os.path.dirname(os.path.realpath(__file__))))
    
    
    print("================================================== è®­ç»ƒå‚æ•°é…ç½® =====================================================")
    
    # æŒ‰ç±»åˆ«åˆ†ç»„æ‰“å°å‚æ•°
    param_groups = {
        "ğŸ”§ åŸºç¡€è®­ç»ƒå‚æ•°": [
            'batch_size', 'epochs', 'accum_iter', 'seed', 'start_epoch'
        ],
        "ğŸ¤– æ¨¡å‹å‚æ•°": [
            'llama_path', 'max_seq_len', 'max_batch_size', 'device'
        ],
        "ğŸ¯ LoRAå‚æ•°": [
            'w_bias', 'lora_layers', 'lora_rank', 'lora_targets', 
            'lora_alpha', 'expert_num', 'hydra_moe', 'expert_weight'
        ],
        "ğŸ’¬ Prompt Tuningå‚æ•°": [
            'prompt_layers', 'prompt_len'
        ],
        "ğŸ”— Parallel Adapterå‚æ•°": [
            'p_adapter_layers', 'p_adapter_size', 'p_adapter_hydra'
        ],
        "ğŸ›ï¸ é€‚é…å™¨è·¯ç”±å‚æ•°": [
            'swi_x'
        ],
        "âš™ï¸ ä¼˜åŒ–å™¨å‚æ•°": [
            'weight_decay', 'lr', 'blr', 'min_lr', 'warmup_epochs'
        ],
        "ğŸ“Š æ•°æ®é›†å‚æ•°": [
            'data_path', 'val_data_path', 'num_workers'
        ],
        "ğŸ“ è¾“å‡ºå‚æ•°": [
            'output_dir'
        ]
    }
    
    for group_name, param_names in param_groups.items():
        print(f"\n{group_name}:")
        print("-" * 30)
        for param_name in param_names:
            if hasattr(args, param_name):
                value = getattr(args, param_name)
                # æ ¼å¼åŒ–æ˜¾ç¤º
                if isinstance(value, str) and len(value) > 50:
                    # é•¿è·¯å¾„æˆªæ–­æ˜¾ç¤º
                    display_value = value[:30] + "..." + value[-20:]
                else:
                    display_value = value
                print(f"  {param_name:20} = {display_value}")

    # è®¾ç½®éšæœºç§å­
    seed = args.seed + get_rank()
    jt.set_global_seed(seed)
    np.random.seed(seed)

    # åˆå§‹åŒ–æ¨¡å‹
    llama_type = ''
    llama_ckpt_dir = os.path.join(args.llama_path, llama_type)
    llama_tokenzier_path = os.path.join(args.llama_path, 'tokenizer.model')
    model = LLaMA_adapter(args, llama_ckpt_dir, llama_tokenzier_path)
    model.float_auto()
    model.train()
    model.init()
    
    # ç»Ÿè®¡å¯è®­ç»ƒå‚æ•°
    print("================================================== å¯è®­ç»ƒå‚æ•° =====================================================")
    trainable_params_sum = 0
    trainable_params_kv = []
    
    for key, val in model.named_parameters():
        if val.requires_grad:
            trainable_params_kv.append((key, val.shape))
            trainable_params_sum += int(val.numel())
    
    print("å¯è®­ç»ƒå‚æ•°æ€»é‡: {}".format(trainable_params_sum))

    # è®¡ç®—è®­ç»ƒé…ç½®
    eff_batch_size = args.batch_size * args.accum_iter * get_world_size()

    # è®¡ç®—å­¦ä¹ ç‡
    if args.lr is None:
        args.lr = args.blr * eff_batch_size / 256

    print("================================================== è®­ç»ƒé…ç½® =====================================================")
    print("åŸºç¡€å­¦ä¹ ç‡: %.2e" % (args.lr * 256 / eff_batch_size))
    print("å®é™…å­¦ä¹ ç‡: %.2e" % args.lr)
    print("æ¢¯åº¦ç´¯ç§¯æ¬¡æ•°: %d" % args.accum_iter)
    print("æœ‰æ•ˆæ‰¹æ¬¡å¤§å°: %d" % eff_batch_size)

    finetune(args_list, model)

    if jt.in_mpi:
        jt.sync_all(True)

    print(f'==================================================== Job End ====================================================')

if __name__ == '__main__':
    datas = ['AddSub/addsub_1.json', 'AQuA/aqua_1.json', 'gsm8k/gsm8k_1.json', 'MultiArith/multiarith_1.json', 'SingleEq/singleeq_1.json', 'SVAMP/svamp_1.json']
    # datas = ['Debug/2.json']

    args_list = []
    for data in datas:
        data_path = '/root/MoA_Jittor/Data/Dataset/math_commonsense/' + data
        args = prepare_args(data_path)
        args_list.append(args)

    main(args_list)